{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the <start> <end> to the '$'\n",
    "# f2 = open('./input2.txt', 'a')\n",
    "\n",
    "# with open('./input.txt', 'rw') as f:\n",
    "#     line = f.readline()\n",
    "#     while line:\n",
    "#         # print repr(line)\n",
    "#         if '<start>' in line:\n",
    "#             f2.write('$\\r\\n')\n",
    "#         elif '<end>' not in line:\n",
    "#             f2.write(line)\n",
    "#         line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上两块生成新的文件，我把start 和end统一改成了'$'，跑一遍就行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    s_all = f.read()\n",
    "    \n",
    "# s1 = \n",
    "# seq_len = 25\n",
    "# i = 0\n",
    "# data = []\n",
    "# while i + 25 < len(s):\n",
    "#     data.append(list(s[i:i+25]))\n",
    "#     i += 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(s_all)\n",
    "letter2int = {}\n",
    "i = 0\n",
    "for k, v in counter.most_common():\n",
    "    letter2int[k] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_letters = len(letter2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(letter2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501470"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, n_layers, batch_size):\n",
    "        super(LSTM_RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = 0.3)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)).cuda(),\n",
    "                autograd.Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, sentence,hidden):\n",
    "        batch_size = sentence.size()[1]\n",
    "        seq_lens = sentence.size()[0]\n",
    "        \n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        hidden = repackage(hidden)\n",
    "        lstm_out, hidden = self.lstm(\n",
    "            embeds.view(seq_lens, batch_size, -1), hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(seq_lens * batch_size, -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage(h):\n",
    "    if type(h) == autograd.Variable:\n",
    "        return autograd.Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train = s_all[:int(len(s_all) * 0.8)]\n",
    "s_valid = s_all[int(len(s_all) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6268.375"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(s_all) * 0.8) / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return idxs\n",
    "\n",
    "def batch_generator(batch_size, chunk_size, s):\n",
    "    i = 0\n",
    "    data = []\n",
    "    while i + chunk_size * batch_size < len(s) - 1:\n",
    "        lines1 = []\n",
    "        lines2 = []\n",
    "        for j in range(batch_size):\n",
    "            lines1.append(s[i + j * chunk_size: i + (j + 1) * chunk_size])\n",
    "            lines2.append(s[i + j * chunk_size + 1: i + (j + 1) * chunk_size + 1])\n",
    "        X = [prepare_sequence(l, letter2int) for l in lines1]\n",
    "        y = [prepare_sequence(l, letter2int) for l in lines2]\n",
    "        X = np.array(X).transpose(1,0)\n",
    "        y = np.array(y).transpose(1,0)\n",
    "        X = torch.LongTensor(X)\n",
    "        y = torch.LongTensor(y)\n",
    "        i += chunk_size * batch_size\n",
    "        X = autograd.Variable(X.cuda())\n",
    "        y = autograd.Variable(y.cuda())\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for a, b in batch_generator(64, 25, s_train):\n",
    "#     print a.size()\n",
    "#     count += 1\n",
    "# print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250.735"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_train) / (64 * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "dim = 93\n",
    "HIDDEN_DIM = 100\n",
    "batch_size = 64\n",
    "chunk_size = 25\n",
    "n_layers = 1\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_batch = len(s_train) / (64 * 25)\n",
    "n_valid_batch = len(s_valid) / (64 * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_RNN(EMBEDDING_DIM, HIDDEN_DIM, n_letters, n_letters, n_layers, batch_size)\n",
    "model = model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = [0, 0, 0]\n",
    "last_valid_loss = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 2.697504114685813\n",
      "valid: 2.337134732851481\n",
      "training: 2.1618541022474975\n",
      "valid: 2.1641180722438236\n",
      "training: 1.9993222275410134\n",
      "valid: 2.091232028766213\n",
      "training: 1.9095292021860393\n",
      "valid: 2.040472807887925\n",
      "training: 1.8505482853646362\n",
      "valid: 2.0057629499629974\n",
      "training: 1.8073251107769464\n",
      "valid: 1.9855350340950972\n",
      "training: 1.7725826125131932\n",
      "valid: 1.975498665013118\n",
      "training: 1.743421329777697\n",
      "valid: 1.968633805509507\n",
      "training: 1.7195645958452035\n",
      "valid: 1.9634771387933683\n",
      "training: 1.700019123742171\n",
      "valid: 1.9595601992428635\n",
      "training: 1.6829565890392049\n",
      "valid: 1.9555783279872019\n",
      "training: 1.6678746534368034\n",
      "valid: 1.9523378055321552\n",
      "training: 1.654074766235242\n",
      "valid: 1.9503894074644421\n",
      "training: 1.6418968632534678\n",
      "valid: 1.9470262710985415\n",
      "training: 1.6309851142590734\n",
      "valid: 1.943496822238736\n",
      "training: 1.6212407595605383\n",
      "valid: 1.939254870435558\n",
      "training: 1.6118535135508671\n",
      "valid: 1.9347231326332466\n",
      "training: 1.6031787999618097\n",
      "valid: 1.931907447582014\n",
      "training: 1.5952781099815474\n",
      "valid: 1.9307482351222123\n",
      "training: 1.5878343999589084\n",
      "valid: 1.9313425989900483\n",
      "training: 1.5811611513156112\n",
      "valid: 1.9307968383377674\n",
      "training: 1.5746940579360071\n",
      "valid: 1.931021943663729\n",
      "training: 1.5686398660549221\n",
      "valid: 1.931731380974067\n",
      "early stopped\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "#inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "# print(tag_scores)\n",
    "\n",
    "for epoch in range(n_epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    \n",
    "    losses = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for X, y in batch_generator(batch_size, chunk_size, s_train):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        # sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        # targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores, hidden = model(X, hidden) # X - 25 * 64,\n",
    "        tag_scores = tag_scores.view(-1, dim).cuda() \n",
    "        y = y.contiguous().view(-1).cuda() # convert label y to contiguous\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        \n",
    "        loss = loss_function(tag_scores, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses += loss.data[0]\n",
    "    \n",
    "    losses_valid = 0 \n",
    "    for X, y in batch_generator(batch_size, chunk_size, s_valid):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        # sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        # targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores, hidden = model(X, hidden)\n",
    "        tag_scores = tag_scores.view(-1, dim).cuda()\n",
    "        y = y.contiguous().view(-1).cuda()\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        \n",
    "        loss = loss_function(tag_scores, y)\n",
    "        losses_valid += loss.data[0]\n",
    "    \n",
    "    record = record[1:]\n",
    "    if losses_valid < last_valid_loss:\n",
    "        record.append(1)\n",
    "    else:\n",
    "        record.append(-1)\n",
    "    if sum(record) == -3:\n",
    "        print ('early stopped')\n",
    "        break\n",
    "    last_valid_loss = losses_valid\n",
    "        \n",
    "    print ('training:', losses / n_train_batch)\n",
    "    print ('valid:', losses_valid / n_valid_batch)\n",
    "\n",
    "# See what the scores are after training\n",
    "# inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "# print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401176"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403200"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "252 * 64 * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16128"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "252 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "   24     1     0  ...     37    18    44\n",
       "    0     0     8  ...     78     4     7\n",
       "   25    40     5  ...     58     9    47\n",
       "       ...          ⋱          ...       \n",
       "    5    40     3  ...      7     9     4\n",
       "    4     8     6  ...     50    28     0\n",
       "    0    21     0  ...      9    13     1\n",
       "[torch.cuda.LongTensor of size 25x64 (GPU 0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  0\n",
       "  0\n",
       "  8\n",
       " ⋮ \n",
       " 18\n",
       " 44\n",
       "  0\n",
       "[torch.cuda.LongTensor of size 1600 (GPU 0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1600, 93])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1600])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2letter = {val: key for key, val in letter2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2letter[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence2(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prime_str='$', predict_len=100, temperature=0.5):\n",
    "    hidden = model.init_hidden(1)\n",
    "    prime_input = prepare_sequence2(prime_str*1, letter2int)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[p], hidden)\n",
    "    inp = autograd.Variable(prime_input.cuda())\n",
    "    inp = inp.view(1,-1)\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        # return top_i\n",
    "        # Add predicted character to string and use as next input\n",
    "        temp = int2letter[top_i[0]]\n",
    "        if temp==\"$\":\n",
    "            break\n",
    "        predicted += temp\n",
    "        inp = autograd.Variable(prepare_sequence2(temp, letter2int).cuda())\n",
    "        inp = inp.view(1,-1)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generate(model, prime_str='<', predict_len=1000, temperature=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>\n",
      "X:14\n",
      "T:Maightar Mofd | Bc d>e:|2 AGD D2F||\n",
      "|:gf/e/ | edc BG~G2 GB/A/|dB e/f/a/g/|agc ge|fe edf:|2 fdc AcBA|AG~E2 EGB|1 B2 cA|BG Bd|ge/f/c/ | Bc/B/A/ Bc | BG~G2 Bdf|A>BcB | AGE D2:|2 AGEF GEF|DEFDEFAF DFA|\n",
      "GFA dcc|Bcd dAG2|fef ef/e/|dB BA|BA B2A d2d|B>A:|2 cd ec|dB AB/c/|dc d2:|\n",
      "|:fa/f/e/f/ g/e/f/f/f/ ge/g/|1 ange March-30\n",
      "M:2/4\n",
      "L:1/8\n",
      "K:F\n",
      "def ge|AB ~e3g|ge/d/c/ BA||\n",
      "<end>\n",
      "<start>\n",
      "X:24\n",
      "T:Cad.\n",
      "S:Storge dourt>\n",
      "X:52\n",
      "T:Ore the McMalan 2 cB|BA BG~G2 G>A:|\n",
      "|:faf gecA|Bc|ed/c/|dB AB/A/G/E/|EG FD|ED DE|FAFA G>A|\n",
      "A2A BeedB dBAG |\n",
      "B2 B2 G>A | B2 BG AB/A/ G>A G>A|ABdg ged|ed BB|AB cA|BA/B/|AB|cd/e/ dB|A>B cB|AFD D3:|\n",
      "<end>\n",
      "<start>\n",
      "X:23\n",
      "T:Battart>\n",
      "X:99\n",
      "T:Bally me norred/B/ AB|cdB ABcA|GBd freengain\n",
      "S:Sess of March-30\n",
      "M:2/4\n",
      "L:1/8\n",
      "K:D\n",
      "dB/e/|dB BA|B2 B>c|BG GE:|2 F2 D2:|\n",
      "<end>\n",
      "<start>\n",
      "X:80\n",
      "T:Paddy Frorickated ||\n",
      "|:fd de/|dB AB/c/|dB GEF|Agfe fed|1 ed BA|B2 de/d/d/ BA|BAFA|~E3/2d/2 e/d/f/e/d/ c/f/g/e/d/|1 d/e/f/g/|de/f/ | ed ed |\n",
      "ed/B/ AG |1 GAG F2G2|G2e2 ged|ecA ABd|e>e ef/e/f/|ge/e/ dc|Bd|fdB d\n"
     ]
    }
   ],
   "source": [
    "print (pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
